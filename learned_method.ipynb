{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from learnedMethodForHologram import *\n",
    "from learnedMethodForHologram.optics import bandLimitedAngularSpectrumMethod as BLASM_v1\n",
    "from learnedMethodForHologram.bandlimited_angular_spectrum_approach import (\n",
    "    bandLimitedAngularSpectrumMethod as BLASM_v2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the .exr data to torch tensors\n",
    "data_processor.read_exr_in_multi_folders(\n",
    "    \"data\\\\test_192\", channlesNum=3, height=192, width=192\n",
    ")\n",
    "data_processor.read_exr_in_multi_folders(\n",
    "    \"data\\\\test_384\", channlesNum=3, height=384, width=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_processor.data_loader(\n",
    "    \"data\\\\test_192\\\\amp.bin\",\n",
    "    \"data\\\\test_192\\\\phs.bin\",\n",
    "    \"data\\\\test_192\\\\img.bin\",\n",
    "    \"data\\\\test_192\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp, phs, img, depth = next(iter(dataLoader))\n",
    "utilities.multi_channel_plotter(utilities.tensor_normalizor_2D(amp), title=None, rgb_img=True)\n",
    "utilities.multi_channel_plotter(utilities.tensor_normalizor_2D(phs), title=None, rgb_img=True)\n",
    "utilities.multi_channel_plotter(utilities.tensor_normalizor_2D(img), title=None, rgb_img=True)\n",
    "utilities.multi_channel_plotter(utilities.tensor_normalizor_2D(depth), title=None, rgb_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp, phs, img, depth = next(iter(dataLoader))\n",
    "depth_np = depth.squeeze().numpy()\n",
    "print(depth_np.max(), depth_np.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import *\n",
    "\n",
    "dataset = data_processor.data_loader_A_B(\n",
    "    path_A = \"data\\\\MIT_CGH_192_bin\\\\train\\\\img.bin\",\n",
    "    path_B = \"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    padding=False,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "%timeit img_depth_tensor = next(iter(dataLoader))\n",
    "img_depth_tensor = next(iter(dataLoader))\n",
    "\n",
    "print(f\"the max value of the img is {img_depth_tensor[:,0].max()}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_loader_for_percepetual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_for_percepetual_loss\n",
    "from learnedMethodForHologram import utilities\n",
    "\n",
    "# \"data\\\\test_192\\\\amp.bin\",\n",
    "# \"data\\\\test_192\\\\phs.bin\",\n",
    "# \"data\\\\test_192\\\\img.bin\",\n",
    "# \"data\\\\test_192\\\\depth.bin\",\n",
    "\n",
    "dataset = data_loader_for_percepetual_loss(\n",
    "    path_img=\"data\\\\test_192\\\\img.bin\",\n",
    "    path_phs=\"data\\\\test_192\\\\phs.bin\",\n",
    "    path_depth=\"data\\\\test_192\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=False,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "amp_phs_depth_tensor = next(iter(dataLoader))\n",
    "\n",
    "print(amp_phs_depth_tensor[:, :3].shape)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    utilities.tensor_normalizor_2D(amp_phs_depth_tensor[:, :3]),\n",
    "    titles=None,\n",
    "    rgb_img=True,\n",
    ")\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    utilities.tensor_normalizor_2D(amp_phs_depth_tensor[:, 3:-1]),\n",
    "    titles=None,\n",
    "    rgb_img=True,\n",
    ")\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(amp_phs_depth_tensor[:, -1]),\n",
    "    title=None,\n",
    "    color=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about band_limited_angular_spectrum_method(V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "distances = torch.tensor(\n",
    "    [0.0, 2.5e-2, 5.0e-3, 7.5e-3, 1.0e-2, 5.0e-2, 10.0e-2, 20.0e-3]\n",
    ")\n",
    "propagator = BLASM_v1(distances=distances)\n",
    "%timeit propagator.band_limited_angular_spectrum_multichannels()\n",
    "\n",
    "del(propagator)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "distances = torch.linspace(0, 2.5e-3, 3)\n",
    "propagator = BLASM_v1(distances=distances,band_limit=True)\n",
    "g_z_complex = propagator.band_limited_angular_spectrum_multichannels()\n",
    "intensity = utilities.intensity_calculator(g_z_complex, True)\n",
    "utilities.multi_sample_plotter(utilities.tensor_normalizor_2D(intensity), titles=None, rgb_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about band_limited_angular_spectrum_method(V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import *\n",
    "test_bandlimited_agular_spectrum_approach.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset_192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import *\n",
    "\n",
    "dataset = data_processor.data_loader(\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\amp.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\phs.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\img.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "sampleX = next(iter(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_tensor = sampleX[0]\n",
    "phase_tensor = sampleX[1]\n",
    "distances = torch.linspace(-2e-4, 2e-4, 7)\n",
    "# spacial_frequency_filter = (\n",
    "#     utilities.generate_custom_frequency_mask(\n",
    "#         sample_row_num=2400,\n",
    "#         sample_col_num=4094,\n",
    "#         x=800,\n",
    "#         y=int(800 * 4094 / 2400),\n",
    "#     )\n",
    "# )\n",
    "propagator = bandlimited_angular_spectrum_approach.bandLimitedAngularSpectrumMethod(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    ")\n",
    "intensities = propagator(\n",
    "    amplitute_tensor=amplitude_tensor,\n",
    "    phase_tensor=phase_tensor,\n",
    "    distances=distances,\n",
    "    # spacial_frequency_filter=spacial_frequency_filter,\n",
    ")\n",
    "normalized_intensities = utilities.tensor_normalizor_2D(intensities)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    normalized_intensities,\n",
    "    titles=None,\n",
    "    rgb_img=True,\n",
    "    save_dir=\"output\\\\test_output\\\\blasm_v2_dataset_192\",\n",
    "    color=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import *\n",
    "\n",
    "dataset = data_processor.data_loader(\n",
    "    \"data\\\\MIT_CGH_384_bin\\\\test\\\\amp.bin\",\n",
    "    \"data\\\\MIT_CGH_384_bin\\\\test\\\\phs.bin\",\n",
    "    \"data\\\\MIT_CGH_384_bin\\\\test\\\\img.bin\",\n",
    "    \"data\\\\MIT_CGH_384_bin\\\\test\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=384,\n",
    "    width=384,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "sampleX = next(iter(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_tensor = sampleX[0]\n",
    "phase_tensor = sampleX[1]\n",
    "distances = torch.linspace(-0.15e-4, 10e-4, 7)\n",
    "# spacial_frequency_filter = (\n",
    "#     utilities.generate_custom_frequency_mask(\n",
    "#         sample_row_num=2400,\n",
    "#         sample_col_num=4094,\n",
    "#         x=800,\n",
    "#         y=int(800 * 4094 / 2400),\n",
    "#     )\n",
    "# )\n",
    "propagator = bandlimited_angular_spectrum_approach.bandLimitedAngularSpectrumMethod(\n",
    "    sample_row_num=384,\n",
    "    sample_col_num=384,\n",
    "    pixel_pitch=8e-6,\n",
    "    wave_length=torch.tensor([638e-9, 520e-9, 450e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    ")\n",
    "intensities = propagator(\n",
    "    amplitute_tensor=amplitude_tensor,\n",
    "    phase_tensor=phase_tensor,\n",
    "    distances=distances,\n",
    "    # spacial_frequency_filter=spacial_frequency_filter,\n",
    ")\n",
    "normalized_intensities = utilities.tensor_normalizor_2D(intensities)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    normalized_intensities,\n",
    "    titles=None,\n",
    "    rgb_img=True,\n",
    "    save_dir=\"output\\\\test_output\\\\blasm_v2_dataset_384\",\n",
    "    color=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about BLASM_v2 and v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learnedMethodForHologram.bandlimited_angular_spectrum_approach import (\n",
    "    bandLimitedAngularSpectrumMethod as BLASM_v2,\n",
    ")\n",
    "from learnedMethodForHologram.bandlimited_angular_spectrum_approach import (\n",
    "    bandLimitedAngularSpectrumMethod_for_single_fixed_distance as BLASM_v3,\n",
    ")\n",
    "\n",
    "propagator_v2 = BLASM_v2(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    ")\n",
    "\n",
    "w_grid_v2 = propagator_v2.generate_w_grid()\n",
    "transfer_function_v2 = propagator_v2.generate_transfer_function(distances=torch.tensor([2.5e-3]))\n",
    "print(transfer_function_v2.shape)\n",
    "\n",
    "propagator_v3 = BLASM_v3(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    "    distance=torch.tensor([2.5e-3]),\n",
    ")\n",
    "\n",
    "w_grid_v3 = propagator_v3.generate_w_grid()\n",
    "transfer_function_v3 = propagator_v3.generate_transfer_function()\n",
    "print(transfer_function_v3.shape)\n",
    "\n",
    "assert torch.allclose(w_grid_v2, w_grid_v3)\n",
    "assert torch.allclose(transfer_function_v2, transfer_function_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset_192 with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import *\n",
    "\n",
    "dataset = data_processor.data_loader(\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\amp.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\phs.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\img.bin\",\n",
    "    \"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    padding=True, # padding the input data to 256x256\n",
    "    cuda=False,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "sampleX = next(iter(dataLoader))\n",
    "\n",
    "print(sampleX[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_tensor = sampleX[0]\n",
    "phase_tensor = sampleX[1]\n",
    "distances = torch.linspace(-1.5e-4, 1.5e-4, 7)\n",
    "# spacial_frequency_filter = (\n",
    "#     utilities.generate_custom_frequency_mask(\n",
    "#         sample_row_num=2400,\n",
    "#         sample_col_num=4094,\n",
    "#         x=800,\n",
    "#         y=int(800 * 4094 / 2400),\n",
    "#     )\n",
    "# )\n",
    "propagator = bandlimited_angular_spectrum_approach.bandLimitedAngularSpectrumMethod(\n",
    "    sample_row_num=512,\n",
    "    sample_col_num=512,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    ")\n",
    "intensities = propagator(\n",
    "    amplitute_tensor=amplitude_tensor,\n",
    "    phase_tensor=phase_tensor,\n",
    "    distances=distances,\n",
    "    # spacial_frequency_filter=spacial_frequency_filter,\n",
    ")\n",
    "normalized_intensities = utilities.tensor_normalizor_2D(intensities)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    normalized_intensities,\n",
    "    titles=None,\n",
    "    rgb_img=True,\n",
    "    save_dir=\"output\\\\test_output\\\\blasm_v2_dataset_192_padding_512\",\n",
    "    color=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import (\n",
    "    bandlimited_angular_spectrum_approach,\n",
    "    neural_network_components,\n",
    ")\n",
    "\n",
    "propagator = bandlimited_angular_spectrum_approach.bandLimitedAngularSpectrumMethod(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    ")\n",
    "\n",
    "amps = torch.randn(4, 3, 192, 192)\n",
    "phs = torch.randn(4, 3, 192, 192)\n",
    "\n",
    "intensities = propagator(amps, phs, torch.tensor([0.0]))\n",
    "print(f\"intensities shape: {intensities.shape}\")\n",
    "\n",
    "amps = torch.randn(1, 3, 192, 192)\n",
    "phs = torch.randn(1, 3, 192, 192)\n",
    "\n",
    "intensities = propagator(amps, phs, torch.tensor([0.0]))\n",
    "print(f\"intensities shape: {intensities.shape}\")\n",
    "\n",
    "amps_phs = torch.randn(1, 6, 192, 192)\n",
    "\n",
    "amp_phs = propagator.propagate_AP2AP(amps_phs, torch.tensor([0.0]))\n",
    "print(f\"propagate_AP2AP shape: {amp_phs.shape}\")\n",
    "\n",
    "amps = torch.randn(1, 3, 192, 192)\n",
    "phs = torch.randn(1, 3, 192, 192)\n",
    "\n",
    "intensities = propagator.propagate_P2I(phs, torch.tensor([0.0]))\n",
    "print(f\"propagate_P2I shape: {intensities.shape}\")\n",
    "\n",
    "amps_phs = torch.randn(4, 6, 192, 192)\n",
    "\n",
    "amp_phs = propagator.propagate_AP2AP(amps_phs, torch.tensor([0.0]))\n",
    "print(f\"propagate_AP2AP shape: {amp_phs.shape}\")\n",
    "\n",
    "intensities = propagator.propagate_P2I(phs, torch.tensor([0.0]))\n",
    "print(f\"propagate_P2I shape: {intensities.shape}\")\n",
    "\n",
    "tensor1 = torch.randn(4, 4, 192, 192)\n",
    "model = neural_network_components.UNet()\n",
    "output = model(tensor1)\n",
    "print(f\"output shape of the UNet: {output.shape}\")\n",
    "\n",
    "tensor2 = torch.randn(4, 2, 192, 192)\n",
    "model = neural_network_components.ResNet_POH()\n",
    "output = model(tensor2)\n",
    "print(f\"output shape of the ResNet_POH: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import (\n",
    "    bandlimited_angular_spectrum_approach,\n",
    "    neural_network_components,\n",
    ")\n",
    "\n",
    "propagator = bandlimited_angular_spectrum_approach.bandLimitedAngularSpectrumMethod_for_single_fixed_distance(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    pixel_pitch=3.74e-6,\n",
    "    wave_length=torch.tensor([639e-9, 515e-9, 473e-9]),\n",
    "    band_limit=False,\n",
    "    cuda=False,\n",
    "    distance=torch.tensor([0.0]),\n",
    ")\n",
    "\n",
    "amps = torch.randn(4, 3, 192, 192)\n",
    "phs = torch.randn(4, 3, 192, 192)\n",
    "\n",
    "intensities = propagator(amps, phs)\n",
    "print(f\"intensities shape: {intensities.shape}\")\n",
    "\n",
    "amps = torch.randn(1, 3, 192, 192)\n",
    "phs = torch.randn(1, 3, 192, 192)\n",
    "\n",
    "intensities = propagator(amps, phs)\n",
    "print(f\"intensities shape: {intensities.shape}\")\n",
    "\n",
    "amps_phs = torch.randn(1, 6, 192, 192)\n",
    "\n",
    "amp_phs = propagator.propagate_AP2AP(amps_phs)\n",
    "print(f\"propagate_AP2AP shape: {amp_phs.shape}\")\n",
    "\n",
    "amps = torch.randn(1, 3, 192, 192)\n",
    "phs = torch.randn(1, 3, 192, 192)\n",
    "\n",
    "intensities = propagator.propagate_P2I(phs)\n",
    "print(f\"propagate_P2I shape: {intensities.shape}\")\n",
    "\n",
    "amps_phs = torch.randn(4, 6, 192, 192)\n",
    "\n",
    "amp_phs = propagator.propagate_AP2AP(amps_phs)\n",
    "print(f\"propagate_AP2AP shape: {amp_phs.shape}\")\n",
    "\n",
    "intensities = propagator.propagate_P2I(phs)\n",
    "print(f\"propagate_P2I shape: {intensities.shape}\")\n",
    "\n",
    "tensor1 = torch.randn(4, 4, 192, 192)\n",
    "model = neural_network_components.UNet()\n",
    "output = model(tensor1)\n",
    "print(f\"output shape of the UNet: {output.shape}\")\n",
    "\n",
    "tensor2 = torch.randn(4, 2, 192, 192)\n",
    "model = neural_network_components.ResNet_POH()\n",
    "output = model(tensor2)\n",
    "print(f\"output shape of the ResNet_POH: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network_components.UNet()\n",
    "model.eval()\n",
    "%timeit output = model(tensor1)\n",
    "\n",
    "model = neural_network_components.ResNet_POH()\n",
    "%timeit output = model(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "from learnedMethodForHologram.watermelon_v1 import watermelon_v1\n",
    "\n",
    "batch_num = 4\n",
    "dataset = data_loader_A_B(\n",
    "    path_A=\"data\\\\MIT_CGH_192_bin\\\\train\\\\img.bin\",\n",
    "    path_B=\"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_num,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "batch = next(iter(dataLoader))\n",
    "model = watermelon_v1(\n",
    "    input_shape=batch.shape,\n",
    "    propagation_distance=torch.tensor([2.5e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "from learnedMethodForHologram.watermelon_v1 import watermelon_v1\n",
    "\n",
    "dataset_train = data_loader_A_B(\n",
    "    path_A=\"data\\\\MIT_CGH_192_bin\\\\test\\\\img.bin\",\n",
    "    path_B=\"data\\\\MIT_CGH_192_bin\\\\test\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataset_test = data_loader_A_B(\n",
    "    path_A=\"data\\\\MIT_CGH_192_bin\\\\validate\\\\img.bin\",\n",
    "    path_B=\"data\\\\MIT_CGH_192_bin\\\\validate\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model = watermelon_v1(\n",
    "    input_shape=(1, 4, 192, 192),\n",
    "    propagation_distance=torch.tensor([2.5e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model.train_model(\n",
    "    train_iter=dataLoader_dataset_train,\n",
    "    test_iter=dataLoader_dataset_test,\n",
    "    num_epochs=10,\n",
    "    lr=1e-2,\n",
    ")\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'watermelon_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import utilities\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "from learnedMethodForHologram.watermelon_v1 import watermelon_v1\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = watermelon_v1(\n",
    "    input_shape=(1, 4, 192, 192),\n",
    "    propagation_distance=torch.tensor([1e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"output\\\\models\\\\watermelon_v2_trainedwith3800samples.pth\"))\n",
    "model.eval()\n",
    "\n",
    "dataset_test = data_loader_A_B(\n",
    "    path_A=\"data\\\\MIT_CGH_192_bin\\\\test\\\\img.bin\",\n",
    "    path_B=\"data\\\\MIT_CGH_192_bin\\\\test\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "batch = next(iter(dataLoader_dataset_test))\n",
    "output = model(batch).to(\"cpu\").detach()\n",
    "print(f\"output type: {type(output)}\")\n",
    "print(f\"output shape: {output.shape}\")\n",
    "print(f\"output max: {output.max()}\")\n",
    "print(f\"output min: {output.min()}\")\n",
    "utilities.multi_sample_plotter(\n",
    "    utilities.tensor_normalizor_2D(output),\n",
    "    titles=[\"reconstructed intensity\"],\n",
    "    rgb_img=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watermelon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "from learnedMethodForHologram.watermelon_v1 import watermelon_v1\n",
    "\n",
    "dataset_train = data_loader_A_B(\n",
    "    path_A=\"data/MIT_CGH_192_bin/test/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/test/depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "# dataset_train = data_loader_A_B(\n",
    "#     path_A=\"data/MIT_CGH_192_bin/train/img.bin\",\n",
    "#     path_B=\"data/MIT_CGH_192_bin/train/depth.bin\",\n",
    "#     samplesNum=3800,\n",
    "#     channlesNum=3,\n",
    "#     height=192,\n",
    "#     width=192,\n",
    "#     cuda=True,\n",
    "# )\n",
    "\n",
    "dataLoader_dataset_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataset_test = data_loader_A_B(\n",
    "    path_A=\"data/MIT_CGH_192_bin/validate/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/validate/depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model = watermelon_v1(\n",
    "    input_shape=(4, 4, 192, 192),\n",
    "    propagation_distance=torch.tensor([1e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model.train_model(\n",
    "    train_iter=dataLoader_dataset_train,\n",
    "    test_iter=dataLoader_dataset_test,\n",
    "    num_epochs=10,\n",
    "    # with v1,v2 lr = 1e-3\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "# save the model\n",
    "# torch.save(model.state_dict(), 'output\\\\models\\\\watermelon_v2_trainedwith3800samples.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "from learnedMethodForHologram.watermelon_v1 import watermelon_v1\n",
    "\n",
    "# load the model\n",
    "model = watermelon_v1(\n",
    "    input_shape=(1, 4, 192, 192),\n",
    "    propagation_distance=torch.tensor([1e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load('output\\\\models\\\\watermelon_v2_trainedwith3800samples.pth'))\n",
    "model.load_state_dict(torch.load('output\\\\models\\\\watermelon_v3_lr_1e-4_.pth'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataset_test = data_loader_A_B(\n",
    "    path_A=\"data/MIT_CGH_192_bin/test/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/test/depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataLoader_dataset_test)).detach()\n",
    "amp_phs_z = model.part1(batch).detach()\n",
    "amp_phs_0 = model.propagator.propagate_AP2AP(amp_phs_z).detach()\n",
    "phs_0 = model.part2(amp_phs_0).detach()\n",
    "intensity = model.propagator.propagate_P2I(phs_0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learnedMethodForHologram import utilities\n",
    "\n",
    "input_phs_restruction = torch.cat((batch[:,0:3],phs_0,intensity),dim=0)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    utilities.tensor_normalizor_2D(input_phs_restruction),\n",
    "    titles = [\"ground_truth_at_z1\",\"POH_at_z0\",\"reconstructed_intensity_at_z1\"],\n",
    "    rgb_img=True,\n",
    "    # save_dir=\"output\\\\images\\\\1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learnedMethodForHologram import utilities\n",
    "\n",
    "input_phs_restruction = torch.cat((batch[:,0:3],phs_0,intensity),dim=0)\n",
    "\n",
    "utilities.multi_sample_plotter(\n",
    "    utilities.tensor_normalizor_2D(input_phs_restruction),\n",
    "    titles = [\"ground_truth_at_z1\",\"POH_at_z0\",\"reconstructed_intensity_at_z1\"],\n",
    "    rgb_img=True,\n",
    "    # save_dir=\"output\\\\images\\\\1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import perceptual_loss\n",
    "from learnedMethodForHologram.data_processor import data_loader_for_percepetual_loss\n",
    "\n",
    "# \"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\"\n",
    "\n",
    "dataset_train = data_loader_for_percepetual_loss(\n",
    "    path_img=\"data\\\\MIT_CGH_192_bin\\\\train\\\\img.bin\",\n",
    "    path_phs=\"data\\\\MIT_CGH_192_bin\\\\train\\\\phs.bin\",\n",
    "    path_depth=\"data\\\\MIT_CGH_192_bin\\\\train\\\\depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataset_validate = data_loader_for_percepetual_loss(\n",
    "    path_img=\"data\\\\MIT_CGH_192_bin\\\\validate\\\\img.bin\",\n",
    "    path_phs=\"data\\\\MIT_CGH_192_bin\\\\validate\\\\phs.bin\",\n",
    "    path_depth=\"data\\\\MIT_CGH_192_bin\\\\validate\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=4, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "validate_iter = torch.utils.data.DataLoader(\n",
    "    dataset_validate, batch_size=1, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "loss_model = perceptual_loss.perceptual_loss(\n",
    "    input_shape=(2, 6, 192, 192),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "loss_model.train_model(\n",
    "    train_iter=train_iter,\n",
    "    test_iter=validate_iter,\n",
    "    num_epochs=30,\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "torch.save(loss_model.state_dict(), 'output\\\\models\\\\perceptual_loss_3800samples_lr4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import perceptual_loss\n",
    "from learnedMethodForHologram.data_processor import data_loader_for_percepetual_loss\n",
    "from learnedMethodForHologram import utilities\n",
    "\n",
    "dataset_test = data_loader_for_percepetual_loss(\n",
    "    path_img=\"data\\\\MIT_CGH_192_bin\\\\test\\\\img.bin\",\n",
    "    path_phs=\"data\\\\MIT_CGH_192_bin\\\\test\\\\phs.bin\",\n",
    "    path_depth=\"data\\\\MIT_CGH_192_bin\\\\test\\\\depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model = perceptual_loss.perceptual_loss(\n",
    "    input_shape=(2, 6, 192, 192),\n",
    "    cuda=True,\n",
    ")\n",
    "model.load_state_dict(torch.load(\"output\\\\models\\\\perceptual_loss_3800samples_lr4.pth\"))\n",
    "\n",
    "n = 15\n",
    "\n",
    "model.eval()\n",
    "depth = model(dataset_test[n][:-1].unsqueeze(0)).detach()\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    depth,\n",
    "    title=\"depth_hat\",\n",
    "    color=2,\n",
    ")\n",
    "\n",
    "print(f\"depth max: {depth.max()}\")\n",
    "print(f\"depth min: {depth.min()}\")\n",
    "print(f\"depth mean: {depth.mean()}\")\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    dataset_test[n][-1],\n",
    "    title=\"Ground Truth\",\n",
    "    color=2,\n",
    ")\n",
    "\n",
    "print(f\"depth max: {dataset_test[0][-1].max()}\")\n",
    "print(f\"depth min: {dataset_test[0][-1].min()}\")\n",
    "print(f\"depth mean: {dataset_test[0][-1].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import (\n",
    "    data_loader_A_B as data_loader_img_depth,\n",
    ")\n",
    "from learnedMethodForHologram.watermelon_v2 import watermelon_v2 as watermelon\n",
    "\n",
    "dataset_train = data_loader_img_depth(\n",
    "    path_A=\"data/MIT_CGH_192_bin/test/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/test/depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataset_train = data_loader_img_depth(\n",
    "    path_A=\"data/MIT_CGH_192_bin/train/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/train/depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataset_validate = data_loader_img_depth(\n",
    "    path_A=\"data/MIT_CGH_192_bin/validate/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/validate/depth.bin\",\n",
    "    samplesNum=100,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "dataLoader_dataset_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "dataLoader_dataset_validate = torch.utils.data.DataLoader(\n",
    "    dataset_validate,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "model = watermelon(\n",
    "    input_shape=(4, 4, 192, 192),\n",
    "    perceptual_model_path=\"output/models/perceptual_loss_3800samples_lr4.pth\",\n",
    "    propagation_distance=torch.tensor([1e-3]),\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model.train_model(\n",
    "    train_iter=dataLoader_dataset_train,\n",
    "    test_iter=dataLoader_dataset_validate,\n",
    "    num_epochs=10,\n",
    "    lr=1e-3,\n",
    "    hyperparameter_lambda = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row_num = 2400\n",
    "sample_col_num = 4094\n",
    "x = sample_row_num // 3\n",
    "y = x * sample_col_num // sample_row_num\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorX = torch.rand(3,5,256,256)\n",
    "tensor_192 = tensorX[\n",
    "    ...,\n",
    "    256//2 - 192//2 : 256//2 + 192//2,\n",
    "    256//2 - 192//2 : 256//2 + 192//2,\n",
    "]\n",
    "print(tensor_192.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram import utilities\n",
    "tensorX = torch.rand(3,5,256,256)\n",
    "%timeit utilities.cut_center_256_192(tensorX).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量并启用梯度计算\n",
    "x = torch.tensor(\n",
    "    [[1.0, 2.0, 3.0], [3.0, 4.0, 5.0], [4.0, 5.0, 6.0]], requires_grad=True\n",
    ")\n",
    "\n",
    "# 对张量进行裁剪，并将裁剪的部分分离出计算图\n",
    "y = x[:1, :].detach()\n",
    "\n",
    "# 进行一些计算\n",
    "z = y.sum() + x.sum()\n",
    "\n",
    "# 反向传播\n",
    "z.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(y.grad)  # 只有没有被分离的部分会有梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量并启用梯度计算\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "# 对张量进行裁剪，并将裁剪的部分分离出计算图\n",
    "x = x[:1, :]\n",
    "x.retain_grad()\n",
    "\n",
    "# 进行一些计算\n",
    "z = x.sum()\n",
    "\n",
    "# 反向传播\n",
    "z.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(x.grad)  # 只有没有被分离的部分会有梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 假设的行数和列数\n",
    "samplingRowNum = 10\n",
    "samplingColNum = 20\n",
    "\n",
    "# 示例张量，假设形状为 (100, 3, 2, 10, 20)\n",
    "amp_phs_tensor = torch.randn(100, 6, samplingRowNum, samplingColNum)\n",
    "\n",
    "# 重新定义形状，并去除大小为1的维度\n",
    "result_tensor = amp_phs_tensor.view(-1, 3, 2, samplingRowNum, samplingColNum)[\n",
    "    :, :, 0\n",
    "].squeeze()\n",
    "\n",
    "print(result_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_loss(output, target):\n",
    "    # 计算每个像素的绝对差异\n",
    "    loss = torch.mean(torch.abs(output - target))\n",
    "    return loss\n",
    "\n",
    "# 假设 batch_num 为 x，图像大小为 192x192，三通道\n",
    "batch_num = 4\n",
    "output = torch.randn(batch_num, 3, 192, 192)  # 模型输出\n",
    "target = torch.randn(batch_num, 3, 192, 192)  # 目标图像\n",
    "\n",
    "# 计算自定义损失\n",
    "loss = custom_loss(output, target)\n",
    "print(f'Custom Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 3, 192, 192)\n",
    "y = x[:,:2]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成示例复光场数据（这里只是一个示例，不是实际数据）\n",
    "# 复光场数据通常是一个 4D 数组，包含振幅和相位信息\n",
    "L = np.random.rand(100, 100, 100, 100) + 1j * np.random.rand(100, 100, 100, 100)\n",
    "\n",
    "# 提取特定视角的信息\n",
    "def extract_view(L, angle_u, angle_v):\n",
    "    # 对复光场数据进行傅里叶变换\n",
    "    L_ft = np.fft.fftshift(np.fft.fft2(L, axes=(0, 1)))\n",
    "    \n",
    "    # 选择特定角度的视角\n",
    "    u_index = int((L.shape[0] / 2) + angle_u * (L.shape[0] / 2))\n",
    "    v_index = int((L.shape[1] / 2) + angle_v * (L.shape[1] / 2))\n",
    "    \n",
    "    # 提取该视角下的图像信息\n",
    "    view = L_ft[u_index, v_index, :, :]\n",
    "    \n",
    "    # 对提取的视角图像进行逆傅里叶变换\n",
    "    view_image = np.fft.ifft2(np.fft.ifftshift(view))\n",
    "    \n",
    "    return np.abs(view_image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 提取不同角度的视角图像\n",
    "angle_u, angle_v = 0.1, 0.2  # 示例角度\n",
    "view_image = extract_view(L, angle_u, angle_v)\n",
    "\n",
    "# 显示视角图像（这里省略具体显示代码，使用 matplotlib 或其他图像显示库）\n",
    "plt.imshow(view_image, cmap='gray')\n",
    "plt.title(f'View at angles ({angle_u}, {angle_v})')\n",
    "plt.show()\n",
    "\n",
    "# 提取不同角度的视角图像\n",
    "angle_u, angle_v = -0.1, -0.2  # 示例角度\n",
    "view_image = extract_view(L, angle_u, angle_v)\n",
    "\n",
    "# 显示视角图像（这里省略具体显示代码，使用 matplotlib 或其他图像显示库）\n",
    "plt.imshow(view_image, cmap='gray')\n",
    "plt.title(f'View at angles ({angle_u}, {angle_v})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some exploration in the frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learnedMethodForHologram.data_processor import data_loader_A_B\n",
    "\n",
    "dataset_train = data_loader_A_B(\n",
    "    path_A=\"data/MIT_CGH_192_bin/train/img.bin\",\n",
    "    path_B=\"data/MIT_CGH_192_bin/train/depth.bin\",\n",
    "    samplesNum=3800,\n",
    "    channlesNum=3,\n",
    "    height=192,\n",
    "    width=192,\n",
    "    cuda=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learnedMethodForHologram import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_spatial_domain = torch.sqrt(dataset_train[0][0:3])\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(dataset_train[0][0:3]),\n",
    "    title=\"image in spatial domain\",\n",
    "    # save_dir=\"output\\images\\lowpass_compare\",\n",
    ")\n",
    "\n",
    "img_freq_domain = torch.fft.fftshift(torch.fft.fft2(img_spatial_domain), dim=(-1, -2))\n",
    "img_freq_domain_amp = torch.abs(img_freq_domain)\n",
    "img_freq_domain_phs = torch.angle(img_freq_domain)\n",
    "img_freq_domain_amp_log = torch.log(img_freq_domain_amp)\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(img_freq_domain_amp_log),\n",
    "    title=\"log amplitude in frequency domain(normalized)\",\n",
    ")\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(img_freq_domain_phs),\n",
    "    title=\"phase in frequency domain\",\n",
    ")\n",
    "print(f\"the max value of the phs is {img_freq_domain_phs.max()}\")\n",
    "print(f\"the min value of the phs is {img_freq_domain_phs.min()}\")\n",
    "\n",
    "# for i in range(3):\n",
    "#     plt.imshow(img_freq_domain_amp_log[i].numpy())\n",
    "#     plt.colorbar()\n",
    "#     plt.title(f\"channel {i}\")\n",
    "#     plt.show()\n",
    "\n",
    "mask = torch.fft.fftshift(utilities.generate_circular_frequency_mask(192, 192, 60))\n",
    "masked_img_freq_domain = img_freq_domain * mask\n",
    "masked_img_freq_domain_amp = img_freq_domain_amp_log * mask\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(masked_img_freq_domain_amp),\n",
    "    title=\"amplitude in frequency domain after filtering\",\n",
    ")\n",
    "\n",
    "masked_img_spatial_domain = torch.fft.ifft2(\n",
    "    torch.fft.ifftshift(masked_img_freq_domain, dim=(-1, -2))\n",
    ")\n",
    "\n",
    "masked_img_spatial_domain_amp = torch.abs(masked_img_spatial_domain)**2\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(masked_img_spatial_domain_amp),\n",
    "    title=\"amplitude in frequency domain of ground truth\",\n",
    "    # save_dir=\"output\\images\\lowpass_compare\",\n",
    ")\n",
    "\n",
    "masked_img_spatial_domain_img = torch.abs(masked_img_spatial_domain) ** 2\n",
    "\n",
    "utilities.multi_channel_plotter(\n",
    "    utilities.tensor_normalizor_2D(masked_img_spatial_domain_img),\n",
    "    title=\"image in spatial domain after filtering\",\n",
    "    # save_dir=\"output\\images\\lowpass_compare\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learnedMethodForHologram import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "mask = utilities.generate_circular_frequency_mask(192, 192, 60)\n",
    "print(mask[mask == 1.0].shape)\n",
    "\n",
    "mask = torch.fft.fftshift(mask)\n",
    "plt.imshow(mask)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from learnedMethodForHologram import utilities\n",
    "\n",
    "\n",
    "def generate_circle_frequency_mask(\n",
    "    sample_row_num=192,\n",
    "    sample_col_num=192,\n",
    "    radius=60,\n",
    "    decay_rate=None,\n",
    "):\n",
    "    shorter_edge = min(sample_row_num, sample_col_num)\n",
    "    if radius > shorter_edge / 2:\n",
    "        raise ValueError(\n",
    "            f\"The radius {radius} is larger than the half of the sample size {shorter_edge/2}\"\n",
    "        )\n",
    "\n",
    "    # Create a grid of (u, v) coordinates\n",
    "    u = torch.fft.fftfreq(sample_row_num).unsqueeze(-1)\n",
    "    v = torch.fft.fftfreq(sample_col_num).unsqueeze(0)\n",
    "    # enable the circle change to ellipse\n",
    "    D = torch.sqrt(u**2 + v**2) * shorter_edge\n",
    "\n",
    "    mask = torch.ones_like(D)\n",
    "    if decay_rate is not None:\n",
    "        # Create the circular low-pass filter with exponential decay around the radius\n",
    "        mask[D > radius] = torch.exp(-decay_rate * (D[D > radius] - radius))\n",
    "    else:\n",
    "        mask[D > radius] = 0.0\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "filter1 = generate_circle_frequency_mask()\n",
    "filter2 = utilities.generate_circular_frequency_mask(192, 192, 60)\n",
    "assert torch.allclose(filter1, filter2)\n",
    "\n",
    "filter = torch.fft.fftshift(filter2)\n",
    "plt.imshow(filter)\n",
    "plt.colorbar()\n",
    "plt.title(\"Filter kernel\")\n",
    "plt.show()\n",
    "\n",
    "cut = filter[95, :]\n",
    "plt.plot(cut)\n",
    "plt.title(\"Filter kernel cut\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(torch.log(cut))\n",
    "plt.title(\"Filter kernel cut\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=12\n",
    "torch.fft.fftfreq(n,1)*n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.fft.fftfreq(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
